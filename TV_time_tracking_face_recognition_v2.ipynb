{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_openface as mo ## custom helper module\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "original_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a predefined model/weights for face recognition ([OpenFace](https://github.com/iwantooxxoox/Keras-OpenFace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mo.model_openface()\n",
    "mo.load_weight_openface(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "img_row = 96\n",
    "img_col = 96\n",
    "box_size_factor = 7.5 # bigger value allows smaller bounding box\n",
    "face_recog_thresh = 0.6\n",
    "\n",
    "db_img_sean = cv2.imread('image_database/sean.jpg')\n",
    "db_img_jason = cv2.imread('image_database/jason.jpg')\n",
    "\n",
    "test_img_sean = cv2.imread('test/sean.jpg')\n",
    "test_img_jason = cv2.imread('test/jason.jpg')\n",
    "test_img_multi = cv2.imread('test/multi.jpg')\n",
    "test_img_james_yena = cv2.imread('test/james_yena.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `detect_face` to detect frontal faces in *gray* and crop in *RGB*\n",
    "## `plot_detect_face` to plot the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(img,fc=face_cascade,flag='db'):\n",
    "    '''\n",
    "    default: flag = 'db' --> assume only one face is present in the image and return only 1 face\n",
    "    flag = 'new' --> if to embed new images (possibly multiple faces)\n",
    "    '''\n",
    "    img_color = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    height,width = img_grey.shape\n",
    "    faces_raw = fc.detectMultiScale(img_grey) # higher accuracy for faces with black glasses\n",
    "    faces = []\n",
    "    for face in faces_raw:\n",
    "        if face[2] > (min(height,width)/box_size_factor):\n",
    "            faces.append(face)\n",
    "    if flag == 'db':\n",
    "        face_box = [0,0,0,0]\n",
    "        for (x, y, w, h) in faces:\n",
    "            if w > face_box[2]:\n",
    "                face_box = [x,y,w,h] # IGNOTE ALL OTHER FALSY FACE BOXES\n",
    "        x,y,w,h = face_box\n",
    "        cropped_list = img_color[y:y+h,x:x+w].copy() # crop without a bounding box \n",
    "        cropped_list = [cropped_list]\n",
    "        img_color = cv2.rectangle(img_color, (x, y), (x+w, y+h), (255,0,0), 4)\n",
    "    if flag == 'new':\n",
    "        cropped_list = []\n",
    "        for (x, y, w, h) in faces: # crop without a bounding box\n",
    "            cropped = img_color[y:y+h,x:x+w].copy()\n",
    "            cropped_list.append(cropped)\n",
    "        for (x, y, w, h) in faces:\n",
    "            img_color = cv2.rectangle(img_color, (x, y), (x+w, y+h), (255,0,0), 4)\n",
    "    return img_color,cropped_list,faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detect_face(img,flag='db'):\n",
    "    '''\n",
    "    plot the full image with a bouding box around the face\n",
    "    plot the cropped faces\n",
    "    default: flag = 'db' --> assume only one face is present in the image and return only 1 face\n",
    "    flag = 'new' --> detect multiple faces with lower accuracy\n",
    "    '''\n",
    "    num_col = 5\n",
    "    result_img,cropped_list,_ = detect_face(img,flag=flag)\n",
    "    plt.axis('off')\n",
    "    plt.title('full image',fontdict={'fontsize':15,'fontweight':'bold'})\n",
    "    plt.imshow(result_img)\n",
    "    if len(cropped_list)==1: \n",
    "        fig,ax=plt.subplots(1,1,figsize=(3,3))\n",
    "        ax.imshow(cropped_list[0])\n",
    "        ax.axis('off')\n",
    "        fig.suptitle('Cropped face image to be embedded',fontsize=15,fontweight='bold')\n",
    "    elif len(cropped_list)<=num_col: \n",
    "        fig,axes=plt.subplots(1,len(cropped_list),figsize=(3*len(cropped_list),3))\n",
    "        for ax,cropped in zip(axes.flatten(),cropped_list):\n",
    "            ax.imshow(cropped)\n",
    "            ax.axis('off')\n",
    "        fig.suptitle('Cropped face image to be embedded (not ordered)',fontsize=15,fontweight='bold')\n",
    "    else:\n",
    "        fig, axes = plt.subplots(int(np.ceil(len(cropped_list)/num_col)),num_col,figsize=(15,3*int(np.ceil(len(cropped_list)/num_col))))\n",
    "        for ax,cropped in zip(axes.flatten(),cropped_list):\n",
    "            ax.imshow(cropped)\n",
    "            ax.axis('off')\n",
    "        fig.suptitle('Cropped face image to be embedded (not ordered)',fontsize=15,fontweight='bold')\n",
    "        if not len(cropped_list)==len(axes.flatten()):\n",
    "            for i in axes.flatten()[len(cropped_list)-len(axes.flatten()):]:\n",
    "                i.set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_detect_face(db_img_sean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detect_face(test_img_multi,flag='new')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, my face was not detected possibly due to the sunglasses! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `embed_image` to embed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_image(face_img,model):\n",
    "    '''\n",
    "    embed the cropped face (input) into 128d vector\n",
    "    use with `detect_face()`\n",
    "    '''\n",
    "    img = cv2.resize(face_img, (img_row,img_col)).astype('float32')\n",
    "    img /= 255.0\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    embedding = model.predict_on_batch(img)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `database_face_embedding` to process and embed images in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_face_embedding():\n",
    "    '''\n",
    "    embed the images in the database\n",
    "    output = {'name':embedding,...}\n",
    "    '''\n",
    "    database_embeddings = {}\n",
    "    os.chdir(os.path.join(os.getcwd(),'image_database'))\n",
    "    for img_file in os.listdir():\n",
    "        name = img_file.split('.')[0]\n",
    "        image_file = cv2.imread(img_file)\n",
    "        _,face_img,_ = detect_face(image_file,fc=face_cascade,flag='db')\n",
    "          \n",
    "        database_embeddings[name] = embed_image(face_img[0], model)\n",
    "    os.chdir(original_path)\n",
    "    return database_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "database_embeddings = database_face_embedding()\n",
    "registered_name = list(database_embeddings.keys())\n",
    "print('Current database contains {} images: \\n{}'.format(len(database_embeddings),[key.upper() for key in registered_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `face_recog` to recognize multiple registered faces from an image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_recog(new_image,database_embeddings,model,size): # now embed_image generate only one set of embed\n",
    "    '''\n",
    "    size --> generate sizeXsize dimension output image\n",
    "    '''    \n",
    "    img_color,faces,faces_coords = detect_face(new_image,flag='new')\n",
    "    new_embeddings = []\n",
    "    for face in faces:\n",
    "        new_embedding = embed_image(face,model)\n",
    "        new_embeddings.append(new_embedding)\n",
    "    name_label = ''\n",
    "    for face_num,(new_embedding,face_coord) in enumerate(zip(new_embeddings,faces_coords)):\n",
    "        min_dist = 100\n",
    "        print('face #{}'.format(face_num+1))\n",
    "        for (registered_name,registered_embedding) in database_embeddings.items():\n",
    "            euc_dist = np.linalg.norm(new_embedding-registered_embedding)\n",
    "            print('Distance from {}: {:.3f}'.format(registered_name,euc_dist))\n",
    "            if euc_dist < min_dist:\n",
    "                min_dist = euc_dist\n",
    "                name = registered_name\n",
    "        if min_dist < face_recog_thresh:\n",
    "            print('***this is {}!***\\n'.format(name.upper()))\n",
    "            name_label = name\n",
    "        else:\n",
    "            print('***not registered!***\\n')\n",
    "            name_label='n/a'\n",
    "        (x, y, w, h) = face_coord\n",
    "        text = '{}.{}'.format(face_num+1,name_label)\n",
    "        (text_width, text_height) = cv2.getTextSize(text, cv2.FONT_HERSHEY_PLAIN,2,5)[0]\n",
    "        text_offset_x = x-3\n",
    "        text_offset_y = y\n",
    "        box_coords = ((text_offset_x, text_offset_y+10), (text_offset_x+text_width,text_offset_y-text_height-10))\n",
    "        if name_label == 'n/a':\n",
    "            img_color = cv2.rectangle(img_color, (x, y), (x+w, y+h), (0,0,255), 4)\n",
    "            img_color = cv2.rectangle(img_color, box_coords[0], box_coords[1], (0,0,255), cv2.FILLED)    \n",
    "        else:\n",
    "            img_color = cv2.rectangle(img_color, (x, y), (x+w, y+h), (255,0,0), 4)\n",
    "            img_color = cv2.rectangle(img_color, box_coords[0], box_coords[1], (255,0,0), cv2.FILLED)\n",
    "        img_color = cv2.putText(img_color,text,(x,y),cv2.FONT_HERSHEY_PLAIN,2,(255,255,255),3)\n",
    "    plt.figure(figsize=(size,size))\n",
    "    plt.imshow(img_color)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on new images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_recog(db_img_jason,database_embeddings,model,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For validation, used the identical picture stored in the database --> 0 distance from *jason* **(model confirmed!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "face_recog(test_img_multi,database_embeddings,model,8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
